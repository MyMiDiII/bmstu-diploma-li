\chapter{\label{design}Конструкторская часть}

\section{Требования и ограничения метода}

Метод построения поисковых индексов в реляционной базе данных на основе глубоких
нейронных сетей (далее – метод построения индексов) должен:

\begin{enumerate}
    \item получать из таблицы реляционной базы данных набор ключей и набор
        соответствующих указателей на записи в индексируемой таблице реляционной
        базы данных или иных значений, выполняющих роль указателей;
    \item выполнять предварительную обработку полученных наборов, такую, как их
        совместную сортировку по значениям ключей, получение позиций ключей в
        отсортированном виде и нормализацию ключей и позиций;
    \item обучать модель нейронной сети на подготовленных набора ключей и
        позиций;
    \item сохранять параметры обученной модели для каждой таблицы с целью
        возможности выполнять запросы поиска без переобучения;
    \item обеспечивать поиск записи (диапазона записей) таблицы по ключу
        (диапазону ключей) с использованием обученной модели;
    \item обеспечивать корректность операции поиска после вставки/удаления новых
        записей путем переобучения модели;
\end{enumerate}

На разрабатываемый метод накладываются следующие ограничения:

\begin{itemize}
    \item в качестве ключей на вход принимаются целые числа для исключения
        решения дополнительной задачи преобразования входных данных;
    \item ключи во входном наборе уникальны.
\end{itemize}

\section{Особенности метода построения индекса}

\subsection{Общее описание метода построения индекса}

Основные этапы метода построения индекса приведены на функциональной
декомпозиции метода на рисунке~\ref{img:idef0-A1}. 

\imgw{idef0-A1}{h!}{17cm}{Функциональная схема метода построения индекса}

На вход методу подается набор уникальных целочисленных ключей, которые перед
обучением модели глубокой нейронной сети проходят предварительную обработку по
определенным правилам, описанным далее. Отдельным этапом выделено получение
значений функций распределения для каждого ключа, относящееся к предварительной
обработке, но представляющее собой ее ключевой момент. Полученные после первых
двух этапов обработанные ключи и соответствующие значения функций используются
для обучение модели глубокой нейронной сети в качестве признаков и меток
соответственно.

Ключевым моментом метода является представление в отсортированном (по ключам)
виде наборов ключей и набора соответвующих указателей на данные. Именно
отсортированный вид позволяет использовать закономерность распределения ключей
по позициям для обучения модели, предсказывать позиции ключей и уточнять их.

Результатом работы метода является структура данных, предствляющая собой индекс
на основе глубокой нейронной сети и имеющая следующие поля:

\begin{itemize}
    \item отсортированный массив ключей, поданных на вход;
    \item отсортированный по значениям ключей массив указателей
        на данные, соответствующие ключам;
    \item модель обученной глубокой нейронной сети, с помощью которой будет
        предсказываться положение ключа в отсортированном массиве;
    \item средняя и максимальная абсолютные ошибки предсказания позиции ключа,
        для ее уточнения и возврата верного указателя на данные.
\end{itemize}

Краткое описания индекса, являющегося результатом работы метода, как структуры
данных представлено на рисунке~\ref{img:index-struct}.

\imgs{index-struct}{h!}{1}{Индекс как структура данных}

Подробное описание каждого этапа приведено в следующих пунктах данного
подраздела.

\subsection{Предварительная обработка данных}

Разрабатываемый метод построения индекса предполагает предварительную обработку
набора целочисленных ключей, схема алгоритма которой представлена на
рисунке~\ref{img:preprocess}.

\imgh{preprocess}{h!}{11cm}{Схема алгоритма предварительной обработки данных}

На вход подаются согласованные массивы ключей и указателей на данные, то есть
считается, что ключ, стоящем на первой позиции в массиве ключей, идентифицирует
данные по указателю, стоящем на первой позиции в массиве указателей; ключ,
стоящей на второй, --- указатель, стоящий на второй, и так далее. С учетом этого
происходит согласованная сортировка двух массивов по значениям ключей.

Далее для последующего обучения модели глубокой нейронной сети производится
нормализация ключей, выступающих в качестве входных данных сети, в дапазон $[0,
1]$, для чего используется метод минимакс-нормализации, при котором
нормализованное значение вычисляется по формуле:

\begin{equation}
    x_{\text{норм}}= \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}},
\end{equation}

где $x_{\text{норм}}$ --- нормализованное значение ключа;

$x$ --- натуральное значение ключа;

$x_{\text{min}},~x_{\text{max}}$ --- минимальное и максимальное возможное
значение ключа в наборе соответственно.

Далее полученный набор ключей размечается путем вычисления для каждого ключа $K$
значения функции распределения $F$ по его позиции $P$ в отсортированном массиве
и количества индексируемых ключей $N$ с помощью формулы:

\begin{equation}
    F(K) = \frac{P}{N}
\end{equation}

На этом предварительная обработка завершается и полученные отсортированные
массивы ключей и указателей, а также соответствующие значения функции
распределения передаются в качестве входных данных на этап обучения модели
глубокой нейронной сети.

Полное описание алгоритма предварительной обработки представлено на
листинге~\ref{alg:preprocess}.

\begin{algorithm}
    \caption{Предварительная обработка данных ~~~~~~~~~~~~~~~~~~~~}
    \label{alg:preprocess}
    \small

    \Input{\\
    \Indp   $keys$~: массив целочисленных ключей;\\
        $data$~: массив указателей на данные, соответствующие ключам;\\
        $N$~: длина массивов.
    }
    \Output{\\
    \Indp $keys$~: отсортированный массив нормализованных ключей;\\
        $data$~: отсортированный по массиву ключей массив указателей;\\
        $cdf$~~: массив значений функции распределения.\\~
    }

    \Begin{
        сортировать $keys$ и $data$ по $keys$\;
        \Comment{здесь и далее под операцией к вектору (массиву) и числу понимается}
        \Comment{применение данной операции с данным числом к каждому элементу вектора}
        \setstretch{1.35}
        $keys \gets \frac{keys - keys[0]}{keys[N - 1] - keys[0]}$\;
        $positions \gets [0, 1, \ldots, N - 1]$\;
        $cdf \gets \frac{positions}{N - 1}$\;

        \Return $keys$, $data$, $cdf$\;
    }
\end{algorithm}

~\\
~\\
~\\
\subsection{Разработка архитектуры глубокой нейронной сети}

Полученные на этапе предварительной обработки массивы ключей и соответствующих
им значений функций распределения используются для обучения глубокой нейронной
сети. Массив данных, хранимый в индексе, используется для возврата нужных
данных при поиске, но не для обучения.

Задача построения индекса на основе нейронной сети сводится к задаче
аппроксимации функции распределения, для решения которой подходят полносвязные
нейронные сети.

За основу архитектуры глубокой нейронной сети принята архитектура из
исследования~\cite{main}, которая представлена на рисунке~\ref{img:fcnn2}.  Это
полносвязная нейронная сеть с двумя скрытыми слоями по 32~нейрона.  В качестве
фунцкции активации в каждом нейроне скрытых слоев используется линейный
выпрямитель или ReLU (\textit{Rectified Linear Unit}), значение которой
вычисляется по формуле~\ref{eq:eq01}.

\imgs{fcnn2}{h!}{1}{Полносвязная нейронная сеть с двумя скрытыми слоями}

\begin{equation}\label{eq:eq01}
    f(x) = \max (0, x).
\end{equation}

Активационная функция выходного слоя является линейной.

~\\

Для исследования возможности увеличения точности предсказания, и как следствие
уменьшение времени поиска, в качестве модели глубокой нейронной сети,
представляющей основу индекса, используется полносвязная нейронная сеть с тремя
слоями, представленная на рисунке~\ref{img:fcnn3}. Число нейронов в слоях и
активационные функции приняты такими же, как в случае глубокой нейронной сети с
двумя скрытыми слоями.

\imgs{fcnn3}{h!}{1}{Полносвязная нейронная сеть с тремя скрытыми слоями}

Обучение обоих моделей глубокой нейронной сети начинается с инициализации весов
случайно сгенерированными значениями по распределению~$U(-\frac{1}{\sqrt{N}},
\frac{1}{\sqrt{N}})$. Собственно обучение проводится методом стохастического
градиентного спуска с оптимизацией в качестве функции потерь
среднеквадратической ошибки (\textit{MSE --- mean squared error}). Описание
алгоритма обучения приведено на листинге~\ref{alg:sdg}. 

\begin{algorithm}[H]
    \caption{Алгоритм обучения глубокой нейронной сети на основе градиентного
    спуска}
    \label{alg:sdg}
    \small

    \Input{\\
    \Indp $keys$~~: массив нормализованных ключей;\\
          $cdf$~~~: массив значений функции распределения для каждого ключа;\\
          $N$~~~~~: длина массивов;\\
          $epochs$~: количество эпох;\\
          $alpha$~: скорость обучения.
    }
    \Output{\\
    \Indp $model$~: обученная модель.\\
    }

    \Begin{
        $model \gets \text{структура модели}$\;
        \Comment{вектор смещений включей в матрицу весов}
        $model.weights \gets U(-\frac{1}{\sqrt{N}}, \frac{1}{\sqrt{N}})$
        \Comment*[r]{случайные значения из распределения}

        \For{$epoch \gets 1$ \KwTo $epochs$}
        {
            согласованно перемешать $keys$ и $cdf$\;

            \For{$i \gets 0$ \KwTo $N - 1$}
            {
                $\hat y = model.predict(key[0])$ \Comment*[r]{прямой проход}
                $mse = (\hat y - cdf[i])^2$\;
                $gradients \gets backward\_pass(model, mse)$ \Comment*[r]{обратный проход}
                $model.weights \gets model.weights - alpha \cdot gradients$
            }
        }
    }


\end{algorithm}

Так как в случае поискового индекса глубокая нейронная сеть будет предсказывать
положения только тех ключей, на которых она обучалась, явления переобучения
нейронной сети является положительным, поэтому в качестве одного батча выступает
одна пара (ключ; значение функции распределения), что также отражено на листинге
выше.

\section{Разработка алгоритмов поиска и вставки}
Основной операцией, выполняемой с помощью индекса, является поиск,
функциональная схема выполнения которого представлена на
рисунках~\ref{img:search-A0}-\ref{img:search-A1}.

\imgs{search-A0}{h!}{1}{Функциональная схема нулевого уровня поиска}
\imgw{search-A1}{h!}{17cm}{Функциональная схема первого уровня поиска}

Получаемая позиция требует уточнения, происходящего за счет получаемого в
результате обучения модели максимального отклонения от истинного расположения.
(Алгоритм уточнения???)

Для реализации вставки происходит добавление новых значений ключа и указателя в
существующие массивы, и повторятся алгоритм построения индекса.
(??? не сначала, а со значений параметров уже обученной модели ???)

\section{Данные для обучения и тестирования индекса}

Так как в основе индекса на основе глубоких нейронных сетей лежит аппроксимация
функции распределения ключей, работу разработанного метода необходимо
протестировать на различных законах, перечисленных далее.

\begin{itemize}
    \item Равномерный закон $R[a,b]$, фукнция распределения которого описывается
        формулой~\ref{eq:eq02}.

        \begin{equation}\label{eq:eq02}
            F(x) = \begin{cases}
                0, & \text{если } x < a \\
                \frac{x - a}{b - a}, & \text{если } a \leq x \leq b \\
                1, & \text{если } x > b.
            \end{cases}
        \end{equation}

        Нормализованные ключей лежат в диапазоне $[0, 1]$, значения функции
        распределения за пределами этого диапазона не представляют интереса для
        построения индекса, поэтому можно считать, что функция имеет вид,
        представленный формулой~\ref{eq:eq03}.

        \begin{equation}\label{eq:eq03}
            F(x) = x, \text{  } x \in [0, 1].
        \end{equation}

    \item Нормальный закон $N(\mu, \sigma^2)$, функция распределения которого
        описывается формулой~\ref{eq:eq04}.

        \begin{equation}\label{eq:eq04}
            F(x) = \int_{-\infty}^{x} \frac{1}{{\sigma \sqrt{2\pi}}}
            \exp\left(-\frac{{(t - \mu)^2}}{{2\sigma^2}}\right) dt
        \end{equation}
\end{itemize}

Для тестирования метода построения по заданным законам генерируются значения
ключей, по совокупности которых формируется эмпирическая функция распределения,
как это было описано выше.

Также для проверки работы метода требуется оценить его работоспособность на
реальных данных, в качестве которых выбраны уникальные идентификаторы элементов
из открытого набора географических данных \textit{OpenStreetMap}, или
\textit{OSM}~\cite{osm}, функция распределения которых имеет более сложный вид,
чем основные законы распределения.

Графики функций распределения каждого из набора входных ключей, представлены на
рисунке~\ref{img:cdfs}.

\imgw{cdfs}{h!}{17cm}{Графики функций распределения ключей из наборов данных для
обучения и тестирования}

\section{Разработка архитектуры программного обеспечения}

Метод построения индекса разработан 
